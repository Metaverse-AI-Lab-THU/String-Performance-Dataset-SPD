<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Audio Matters Too! Enhancing Markerless Motion Capture with Audio
  Signals for String Performance Capture.">
  <!-- <meta name="keywords" content="SMPLX, Diffusion, Character Animation"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Audio Matters Too!<br>Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture.</title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h4 class="title conference-title is-4">NeurIPS 2023</h4> -->
            <h2 class="title is-2 publication-title">Audio Matters Too! Enhancing Markerless Motion Capture with Audio
              Signals for String Performance Capture.</h2>
            <div class="is-size-5 publication-authors">

              <span class="author-block">Yitong Jin<sup>1,2,*</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Zhiping Qiu<sup>1,2,*</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Yi Shi<sup>1,2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Shuangpeng Sun<sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Chongwu Wang<sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Donghao Pan<sup>2</sup></span>
              <br />
              <span class="author-block">Jiachen Zhao<sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Zhenghao Liang<sup>3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Yuan Wang<sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Xiaobing Li<sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Feng Yu<sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><a href="https://ytrock.com/">Tao Yu</a><sup>1,✉</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Qionghai Dai<sup>1,✉</sup></span>
              <br />
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tsinghua University</span> &nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Central Conservatory of Music</span> &nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>3</sup>Weilan Tech, Beijing</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution &nbsp;&nbsp;</span>
              <span class="author-block"><sup>✉</sup>Corresponding Author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3658235" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> &nbsp;&nbsp;&nbsp;&nbsp;

                
                <!-- arXiv Link. -->
                <span class="link-block">
                <a href="https://arxiv.org/abs/2405.04963" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-book"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> &nbsp;&nbsp;&nbsp;&nbsp;


                <!-- Video Link. -->
                <span class="link-block">
                <a href="https://www.youtube.com/watch?v=DBhHA1IZWCI" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> &nbsp;&nbsp;&nbsp;&nbsp;

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (comming soon)</span>
                  </a>
                </span> &nbsp;&nbsp;&nbsp;&nbsp;


                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>SPD-dataset (comming soon)</span>
                  </a>
                </span> &nbsp;&nbsp;&nbsp;&nbsp;

                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href="https://drive.google.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <a href ="./static/images/poster.png"><img src="./static/images/poster.png" autoplay muted loop playsinline height="100%"></a>
        <h5 class="subtitle is-5">
          Motion Capture of musical instrument performance is challenging even with markers. By extracting playing cues inherent in the audio for markerless video motion capture, our method recovers subtle finger-string contacts and intricate playing movements. We further contribute the first large-scale String Performance Dataset (SPD) with high-quality motion and contact annotations.
        </h5>
        <img src="./static/images/teaser.jpg" autoplay muted loop playsinline height="80%">
      </div>
    </div>
  </section>

  <p style="margin-bottom: -1.5cm;"></p>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal,
            we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of
            the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio
            signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of
            introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved
            from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced
            from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual
            motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection. It holds significant
            implications and guidance for string instrument pedagogy, animation, and virtual concerts, as well as for both musical performance analysis and generation.
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/DBhHA1IZWCI?si=xgHUI9TFhXfCwm14"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->

    </div>
  </section>


  <!-- <p style="margin-bottom: -0.5cm;"></p> -->

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Examples in SPD</h3>
        <p align="middle">
          <img src="./static/images/cello_all.gif" width="384" height="384">
          <img src="./static/images/violin_all.gif" width="384" height="384">
        </p>
        <!-- <h2 class="subtitle has-text-centered">
          Throw ball        Standing long jump          
        </h2> -->

        <!-- <p align="middle">
          <img src="./static/videos/MoCap_20230422_182056.gif" width="512" height="512">
        </p>
        <h2 class="subtitle has-text-centered">
          Throw ball
        </h2> -->

        <p align="middle">
          <img src="./static/images/cello_hand.gif" width="384" height="384">
          <img src="./static/images/violin_hand.gif" width="384" height="384">
        </p>

        <p style="margin-bottom: 1.0cm;"></p>
        <h3 class="title is-3 has-text-centered">Ablation Study</h3>
        <p align="middle">
          <img src="./static/images/violin_ablation.gif" width="768" height="768">
        </p>
        <!-- <h2 class="subtitle has-text-centered">
          Side step
        </h2> -->

        <!-- <p align="middle">
          <img src="./static/videos/MoCap_20230422_151220.gif" width="512" height="512">
        </p>
        <h2 class="subtitle has-text-centered">
          Rope skipping
        </h2> -->

      </div>
    </div>
  </section>

  <style>
      button {
          position: absolute; /* 绝对定位 */
          right: 10px; /* 向右偏移的距离 */
          top: 10px; /* 向上偏移的距离 */
          padding: 5px 10px;
          border: none;
          background-color: #007bff;
          color: white;
          cursor: pointer;
          border-radius: 4px;
          display: flex;
          align-items: center;
      }
      button span {
          margin-left: 5px;
          opacity: 0; /* 默认隐藏 */
          transition: opacity 0.3s; /* 添加过渡效果 */
      }
      button:hover span {
          opacity: 1; /* 悬停时显示文本 */
      }
      button:hover {
          background-color: #0056b3; /* 悬停效果 */
      }
  </style> 

  <p style="margin-bottom: -1.5cm;"></p>
  <section class="section" id="BibTeX">
    <div class="BibTeX container">
      <h2 class="title">BibTeX</h2>
      <pre><code id="ArXiv_BibTeX">
        <button onclick="copyBibTeX('ArXiv_BibTeX')">
          <i class="fas fa-copy"></i>
          <span Copy BibTeX from ArXiv.></span>
        </button>
        @article{jin2024audio,
        title={Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture},
        author={Jin, Yitong and Qiu, Zhiping and Shi, Yi and Sun, Shuangpeng and Wang, Chongwu and Pan, Donghao and Zhao, Jiachen and Liang, Zhenghao and Wang, Yuan and Li, Xiaobing and others},
        journal={arXiv preprint arXiv:2405.04963},
        year={2024}
  }</code></pre>

      <pre><code id="ACM_BibTeX">
        <button onclick="copyBibTeX('ACM_BibTeX')">
          <i class="fas fa-copy"></i>
          <span Copy BibTeX from ACM Library.></span>
        </button>
        @article{10.1145/3658235,
        author = {Jin, Yitong and Qiu, Zhiping and Shi, Yi and Sun, Shuangpeng and Wang, Chongwu and Pan, Donghao and Zhao, Jiachen and Liang, Zhenghao and Wang, Yuan and Li, Xiaobing and Yu, Feng and Yu, Tao and Dai, Qionghai},
        title = {Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture},
        year = {2024},
        issue_date = {July 2024},
        publisher = {Association for Computing Machinery},
        address = {New York, NY, USA},
        volume = {43},
        number = {4},
        issn = {0730-0301},
        url = {https://doi.org/10.1145/3658235},
        doi = {10.1145/3658235},
        abstract = {In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection. It holds significant implications and guidance for string instrument pedagogy, animation, and virtual concerts, as well as for both musical performance analysis and generation. Our code and SPD dataset are available at https://github.com/Yitongishere/string_performance.},
        journal = {ACM Trans. Graph.},
        month = {jul},
        articleno = {90},
        numpages = {10},
        keywords = {marker-less motion capture, string performance, multi-modality}
}</code></pre>

      

<!-- <h2 class="title">Contact Us</h2>
<style>
  ul {
    list-style-type: circle;
  }
</style> 
<ul>
  <li>For detailed questions about this work, please contact ...</li>
  <li>We will ... </li>
</ul> -->

<!--<h2 class="title">Con</h2>
      
-->
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-size-6">
          <div class="content">
            <p>
              This website is created with this <a href="https://motion-x-dataset.github.io/">template</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

<script>
    function copyBibTeX(id) {
        // 获取BibTeX代码块
        const bibtexBlock = document.getElementById(id);
        // 创建一个临时文本区域
        const textArea = document.createElement('textarea');
        textArea.value = bibtexBlock.innerText; // 获取ACM BibTeX文本
        document.body.appendChild(textArea); // 将文本区域添加到文档中
        textArea.select(); // 选择文本
        document.execCommand('copy'); // 执行复制命令
        document.body.removeChild(textArea); // 移除临时文本区域
        alert('BibTeX 已复制到剪贴板！'); // 提示用户
    }
</script>
  
</html>
